---
title: "Appendix_1_Cass_Edits"
output:
  html_document: default
  pdf_document: default
---
*** 
# Contents


***
## 1.0 Set up the working environment

### 1.1 Load Packages

Running Maxent in R requires several packages. Specifically, the "dismo" package, which contains the functions for maximum entropy species distribution modeling. However, the "rgbif" package gives access to the GBIF database for occurrence location downloads, (https://cran.r-project.org/web/packages/rgbif/rgbif.pdf), the "raster" package provides functions for working with gridded data ( https://cran.r-project.org/web/packages/raster/raster.pdf), the "rgeos" package provides the ability to manipulate spatial data (https://cran.rstudio.com/web/packages/rgeos/rgeos.pdf), and the "knitr" package allows for report generation and easy display of modeling output (https://cran.r-project.org/web/packages/knitr/knitr.pdf), which are needed to complete the modeling process. 

#####Thread 1
```{r setup1,message=FALSE, eval=FALSE}

library("dismo")
library("raster")
require("rgeos")
library("rgbif")
library("knitr")
```

If you are using a Mac machine, an additional step is needed

#####Thread 2
```{r setup2, eval=FALSE}

dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/
   Contents/Home/jre/lib/server/libjvm.dylib')
```

Depending on the data used for SDM, the memory used by Maxent may exceed the limit of java, to address this, the following code will increase the amount of memory used by the Java Virtual Machine (JVM) to heighten Maxent’s performance.

#####Thread 3
```{r setup3,message=FALSE, eval=FALSE}

# Java must be enabled for the knitr package to work
require("rJava")

knitr::opts_knit$set(root.dir = 'G:/_PROJECTS_OTHER/Xiao_R Maxent/output')
```

### 1.2 Set up the Maxent Path

In order for Maxent to work properly in R, the .jar file associated with Maxent needs to be accessible.

#####Thread 4
```{r maxent, eval=FALSE, echo=TRUE}

# download maxent.jar 3.3.3k, and place the file in the desired folder
utils::download.file(url="https://github.com/mrmaxent/Maxent/blob/
                     master/ArchivedReleases/3.3.3k/maxent.jar",
                     destfile=paste0(system.file("java", package="dismo"),
                                     "/maxent.jar"),mode="wb") 
# mode must be wb for binary file, otherwise maxent.jar cannot execute

# also note that both R and Java need to be the same bit (either 32 or 64) 
#   to be compatible to run

# to increase memory size of the JVM and prevent memory issues with Maxent.jar use:
( java.parameters = c("-Xss2560k", "-Xmx2g") ) 

```
***

##2.0 Prepare data input 

###2.1 Load environmental layers  

In our example, we used bioclimatic variables (downloaded from worldclim.org) as input environmental layers for our SDM. We suggest saving all environmental layers in one folder to make access to these easier. We stack our environmental layers so that they may be processed simultaneously (batch processing) to decrease errors that may occur when processed individually.

#####Thread 5
```{r load rasters, eval=FALSE, echo=TRUE}


# This searches for all files that have "data/bioclim/" in the path name and have 
#    a file extension of .bil. You can edit this code to reflect the path name and 
#    file extension for your environmental variables
clim <- list.files("data/bioclim/",pattern=".bil$",full.names = T)

# stacking the bioclim variables to process them at one go 
clim <- raster::stack(clim) 

```

###2.2 Occurrence data

####2.2.1 Download occurrence data 

For our example, the nine banded armadillo, we downloaded occurrence data from GBIF, the Global Biodiversity Information Facility. We have provided an if/else statement that checks for a file with “data/occ-raw” in the pathname. If a file does exist, R will load this file, otherwise it will download occurrence locations for _Dasypus novemcinctus_, the nine banded armadillo, from gbif and save it as a .csv file named “data/occ_raw.csv”.

#####Thread 6
```{r prepare, message=TRUE, warning=FALSE, eval=FALSE}

# If you save your occurrence data using different path names, you can specify this 
#   both in the search and for the saved data file
if(file.exists("data/occ_raw")){
  
  load("data/occ_raw")
}else{

  occ_raw <- gbif("Dasypus novemcinctus")
  save(occ_raw,file = "data/occ_raw")
  write.csv("data/occ_raw.csv")
}


# Check the first few lines of the occurrence dataset for inspection
head(occ_raw)
```

###2.2.2 Clean occurrence data

Since some of our records do not have appropriate coordinates and some have missing locational data, we need to find these records and remove them from our dataset. To do this, we create a new dataset named “occ_clean”, which is a subset of the “occ_raw” dataset where records with missing latitude and/or longitude are removed. This particular piece of code also returns the number of records that were removed from the dataset. Additionally, we remove duplicate records and create a subset of the cleaned data with the duplicates removed. 

#####Thread 7
```{r clean data 1, eval=FALSE}
# remove bad coordinates, where either the lat or long coordinate is missing
occ_clean <- subset(occ_raw,(!is.na(lat))&(!is.na(lon)))
cat(nrow(occ_raw)-nrow(occ_clean), "records are removed")

# remove duplicate data based on latitude and longitude
dups <- duplicated(occ_clean[c("lat","lon")])
occ_unique <- occ_clean[!dups,]
cat(nrow(occ_clean)-nrow(occ_unique), "records are removed")
```

Up to this point we have been working with a data frame, but it has no spatial relationship defined in R, so we needed to make the data spatial. Once our data is spatial we can use the plot() function to see the occurrence data and allow us to check for data points that appear to be erroneous.

#####Thread 8
```{r clean data 2, eval=FALSE}

# make occ spatial
coordinates(occ_unique) <- ~ lon + lat

## look for erroneous points
plot(occ_unique) 

```

In Figure #, we can see several points that appear outside the known distribution of _Dasypus novemcinctus_ (North and South America) and we need to remove these from our occurrence data set. To do this we select points that have longitudes greater than -110 or lower than -40 (outside our area of interest) and remove them from the data set. 

#####Thread 9
```{r clean data 3, eval=FALSE}

# remove some errors
occ_unique <- occ_unique[which(occ_unique$lon>-110 &
                                 occ_unique$lon < -40),]
```

Maxent only utilizes only one occurrence location per pixel or cell for the environmental data when creating models, so we need to thin our occurrence data so that only one location falls within each cell.

#####Thread 10
```{r clean data 4, eval=FALSE}
# thin occ data (keep one occ per cell)
cells <- cellFromXY(clim[[1]],occ_unique)

dups <- duplicated(cells)

occ_final <- occ_unique[!dups,]

cat(nrow(occ_unique)-nrow(occ_final), "records are removed")

# plot the first climatic layer (or )
plot(clim[[1]]) 
# replace [[1]] with any nth number of the layer of interest from the raster stack

# plot the final occurrence data on the enviromental layer
plot(occ_final,add=T,col="red") 
# the 'add=T' tells R to put the incoming data on the existing layer
```

## 2.3 Set up study area

In setting up our study area we adopt a methodology that better samples the background and presence environmental conditions for modeling. we create a buffer around our occurrence locations and define this as our study region, which will allows us to better train the model to distinguish between conditions associated with species presence and background locations without over sampling from the background. We establish a 4 decimal degree buffer around the occurrence points and to make sure that our buffer encompasses the appropriate area, we plot the occurrence points, the first environmental layer, and the buffer polygon.

#####Thread 11
```{r set up study area 1, eval=FALSE}
# this creates a 4 decimal degree buffer around the occurrence data 
occ_buff <- buffer(occ_final,4) 

# plot the first element ([[1]]) in the raster stack
plot(clim[[1]])
# this adds the occurrence data
plot(occ_final,add=T,col="red") 
# this adds the buffer polygon
plot(occ_buff,add=T,col="blue") 
```

With a defined study area and the environmental layers stacked, we then clip the layers to the extent of our study area. However, for ease of processing, we do this in two steps rather than one. First we create a coarse rectangular shaped study area around the occurrence data and study area to reduce environmental data raster size and then extract by mask using the buffer we created to more accurately clip environmental layers. We have found that this approach keeps the computer from slowing down by trying to clip the world extent bioclim data layers to a smaller study area. We save the cropped environmental layers as .asc (ascii files) as inputs for Maxent.

#####Thread 12
```{r set up study area 2, eval=FALSE}
# crop study area to a manageable extent (rectangle shaped)
studyArea <- crop(clim,extent(occ_buff))  

# the 'study area' created by extracting the buffer area from the raster stack
studyArea <- mask(studyArea,occ_buff)
# output will still be a raster stack, just of the study area

# save the new study area rasters as ascii
writeRaster(studyArea,
            filename=paste0("data/studyarea/",names(studyArea),".asc"), 
            #keeps names but adds .asc
            format="ascii", # output format
            bylayer=TRUE, # perform this for all the layers
            overwrite=T)
```

After processing our environmental data, we now need to define our background points and establish training and testing points. We want define a random sample that will be selected each time we used the sampleRandom function in R, so that we can re-run models. This will allow the same set of points to be selected every time the code is run rather than generating a new set of random points. The set.seed(1) function accomplished this. With an established a random selection technique we select 10,000 background points from the study area, ignoring pixels with no data. To visualize the background points selected, we plot the study area, occurrence data, and background data. 

#####Thread 13
```{r set up study area 3, eval=FALSE}
# when the number provided to set.seed() function, the same random sample will be 
# selected in the next line
set.seed(1) 
# use this code before the sampleRandom function everytime, if you want to get
# the same "random samples"

bg <- sampleRandom(x=studyArea,
                   size=10000,
                   na.rm=T,    # na.rm removes the 'Not Applicaple' points 
                   sp=T)       #  sp is telling R to give us spatial points 

plot(studyArea[[1]])
# add the background points to the plotted raster
plot(bg,add=T) 
# add the occurrence data to the plottted raster
plot(occ_final,add=T,col="red") 

```

##2.4 Split occurrence data into training & testing

We then use the same set.seed(1) function to select 50% of our data. We define the first 50% selected as training data and the other 50% as testing. 

#####Thread 14
```{r cut occ into training & testing, eval=FALSE}
# get the same random sample for training and testing
set.seed(1) 

# randomly select 50% for training
selected <- sample(1:nrow(occ_final),nrow(occ_final)*0.5)

# this is the selection
occ_train <- occ_final[selected,] 

# this is the opposite of the selection
occ_test <- occ_final[-selected,]
```

###2.5 Format data for Maxent

The last step before model creation is to format Maxent inputs for modeling since the dismo package requires a table/dataframe for model inputs. We extract environmental data from the raster stack for the backround, training, and testing points in a dataframe format. 

#####Thread 15
```{r prepare data for maxent 1, eval=FALSE}
# extracting env conditions for training data from the raster stack 
#   (i.e multiple columns)
p <- extract(clim,occ_train) 
# the extracted data is returned as a dataframe

# extracting env conditions for testing data
p_test <- extract(clim,occ_test)

# extracting env conditions for background
a <- extract(clim,bg)
```

The dismo package reads a “1” as a presence and “0” as a pseudo-absence. Thus, we need to assign a “1” to the training environmental conditions and a “0” for the background. We first create a set of rows with the same number as the training and testing data, and put the value of “1” in each cell and a “0” for background. We then combine the “1’s” and “0’s” into a vector that will be added to the dataframe containing the environmental conditions associated with the testing and background conditions.

#####Thread 16
```{r prepare data for maxent 2, eval=FALSE}
# repeat the number 1 as many numbers as the number of rows in p
pa <- c(rep(1,nrow(p)), rep(0,nrow(a))) 

# (rep(1,nrow(p)) creating the number of rows as the p data set to have 
#    the number one as the indicator for presence
#  rep(0,nrow(a)) creating the number of rows as the a data set to have 
#    the number zero as the indicator for absence
# The c combines these ones and zeros into a new vector that can be added 
#    to the Maxent table

# data frame with the environmental attributes of the presence and absence locations
pder <- as.data.frame(rbind(p,a)) 
```

##3.0 Maxent models

###3.1 Simple implementation

To run simple distribution models using the “dismo” package, only three function specifications are needed; the climatic conditions, the occurrence data, and an output location for the results. The environmental conditions and occurrence data are the databases we created earlier. Model parameters can also be specified in the maxent function, when not speficied default parameters are used. We provide details about modeling parameters later in this document. You can view model results in an html browser.

#####Thread 17
```{r simple maxent model, eval=FALSE}
# mod <- maxent(x=climatic data, p=occurrence data, path=output location)
mod <- maxent(x=pder, # env conditions
              p=pa,   # 1:presence or 0:absence
              path=paste0(getwd(),"/output/maxent_outputs"), # folder for output
              args=c("responsecurves") # parameter specification
              )

# view the maxent model in a html brower
mod

# view detailed results
mod@results
```

###3.2 Predict function

Once a model has been constructed, it will not provide predictions unless specified in the model parameters. However, by using the predict function we can predict the model to raster layers or occurrence data frames.

#####Thread 18
```{r predict, eval=FALSE}
# example 1, project to study area [raster]
ped1 <- predict(mod,studyArea)
# studyArea is the clipped rasters we used to extract environmental conditions

# plot the continuous prediction
plot(ped1)

# example 2, project to the world (outside the study area)
ped2 <- predict(mod,clim)
plot(ped2)

# example 3, project with training occurrences [dataframes]
ped3 <- predict(mod,p)
head(ped3)

# creates a histogram of the prediction
hist(ped3)
```

###3.3 Model evaluation

To evaluate models, we use the evaluate function from the “dismo” package, which gives model performance using the AUC metric. Training and testing AUC can be calculated using the evaluate function. 

#####Thread 19
```{r model evaluation 1, eval=FALSE}
# using "training data" to evaluate 
mod_eval_train <- dismo::evaluate(p=p,a=a,model=mod) 
#p & a are dataframes, the p and a are the training presence and 
#   training absence points

print(mod_eval_train)
# This is the test AUC

#p & a are dataframes, the p and a are the testing presence and 
#   tesing absence points
mod_eval_test <- dismo::evaluate(p=p_test,a=a,model=mod)  

print(mod_eval_test) # training AUC may be higher than testing AUC
```

To threshold our continuous predictions of suitability into binary predictions we use the threshold function of the “dismo” package.To plot the binary prediction, we plot the predictions that are larger than the threshold.  

#####Thread 20
```{r model evaluation 2, eval=FALSE}

# calculate thresholds of models
thd1 <- threshold(mod_eval_train,"no_omission") 
# 0% omission rate [minimum training presence]

thd2 <- threshold(mod_eval_train,"spec_sens") 
# hiest TSS

# plotting points that are above the previously calculated tresholded value
plot(ped1>=thd1) 
```